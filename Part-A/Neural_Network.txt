import matplotlib.pyplot as plt
import numpy as np
import wandb
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from keras.datasets import fashion_mnist, mnist
import math

wandb.login()

# Load fashion_MNIST data using Keras
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=True, random_state=42)

# Flatten the images
x_train = x_train.reshape(x_train.shape[0], -1)
x_val = x_val.reshape(x_val.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

# Normalize pixel values to be between 0 and 1
x_train = x_train.astype('float32') / 255
x_val = x_val.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# One-hot encode the labels
encoder = OneHotEncoder(sparse_output=False)
y_train = encoder.fit_transform(y_train.reshape(-1, 1))
y_val = encoder.transform(y_val.reshape(-1, 1))
y_test = encoder.transform(y_test.reshape(-1, 1))

class ActivationFunctions:
    def sigmoid(self, x):
        x = np.clip(x, -200, 200)
        return 1 / (1 + np.exp(-x))

    def relu(self, x):
        return np.maximum(x, 0)

    def softmax(self, x):
        max_x = np.max(x, axis=0, keepdims=True)  # Keepdims to maintain dimensions
        exp_x = np.exp(x - max_x)
        sum_exp_x = np.sum(exp_x, axis=0, keepdims=True)  # Keepdims to maintain dimensions
        return exp_x / sum_exp_x

class DifferentialFunctions:
    def sigmoid_derivative(self, x):
        sigmoid = ActivationFunctions().sigmoid(x)
        return sigmoid * (1 - sigmoid)

    def tanh_derivative(self, x):
        return 1 - np.tanh(x) ** 2

    def relu_derivative(self, x):
        return (x > 0).astype('float64')

    def identity_derivative(self, x):
        return np.ones_like(x)

class Initializer:
    def initialize_weights_biases(self, hidden_layers, neurons_per_layer, initialization_method):
        weights = []
        biases = []
        
        if initialization_method == "random":
            std = 0.01  # You can adjust this standard deviation
            weights.append(std * np.random.randn(neurons_per_layer, 784))
            for _ in range(hidden_layers - 1):
                weights.append(std * np.random.randn(neurons_per_layer, neurons_per_layer))
            weights.append(std * np.random.randn(10, neurons_per_layer))

            biases.append(np.zeros((neurons_per_layer, 1)))
            for _ in range(hidden_layers - 1):
                biases.append(np.zeros((neurons_per_layer, 1)))
            biases.append(np.zeros((10, 1)))
        elif initialization_method == "he":
            weights.append(np.random.randn(neurons_per_layer, 784) * np.sqrt(2 / 784))
            for _ in range(hidden_layers - 1):
                weights.append(np.random.randn(neurons_per_layer, neurons_per_layer) * np.sqrt(2 / neurons_per_layer))
            weights.append(np.random.randn(10, neurons_per_layer) * np.sqrt(2 / neurons_per_layer))

            biases.append(np.zeros((neurons_per_layer, 1)))
            for _ in range(hidden_layers - 1):
                biases.append(np.zeros((neurons_per_layer, 1)))
            biases.append(np.zeros((10, 1)))
        else:  # Xavier initialization
            weights.append(np.random.randn(neurons_per_layer, 784) * np.sqrt(1 / 784))
            for _ in range(hidden_layers - 1):
                weights.append(np.random.randn(neurons_per_layer, neurons_per_layer) * np.sqrt(1 / neurons_per_layer))
            weights.append(np.random.randn(10, neurons_per_layer) * np.sqrt(1 / neurons_per_layer))

            biases.append(np.zeros((neurons_per_layer, 1)))
            for _ in range(hidden_layers - 1):
                biases.append(np.zeros((neurons_per_layer, 1)))
            biases.append(np.zeros((10, 1)))

        return weights, biases

class ArithmeticOperations:
    def add(self, u, v):
        return [u[i] + v[i] for i in range(len(u))]

    def subtract(self, v, dv, eta):
        return [v[i] - (eta * dv[i]) for i in range(len(v))]

    def rmsprop_subtract(self, v, dv, lv, eps, eta):
        new_v = []
        for i in range(len(v)):
            if v[i] is not None:
                ueta = eta / (np.sqrt(np.sum(lv[i])) + eps)
                new_v.append(v[i] - (ueta * dv[i]))
            else:
                new_v.append(None)
        return new_v

    def adam_subtract(self, V, mV_hat, vV_hat, eps, eta):
        new_V = []
        for i in range(len(V)):
            if V[i] is not None:
                norm = np.linalg.norm(vV_hat[i])
                ueta = eta / (np.sqrt(norm) + eps)
                new_V.append(V[i] - (ueta * mV_hat[i]))
            else:
                new_V.append(None)
        return new_V

class NeuralNetwork:
    def __init__(self, input_size, output_size, config):
        self.input_size = input_size
        self.output_size = output_size
        self.layers = config['layers']
        self.activation = config['activation']
        self.neurons_per_layer = config["neurons_per_layer"]
        self.learning_rate = config["learning_rate"]
        self.batch_size = config["batch_size"]
        self.initialization = config["Initialization"]
        self.config = config

        self.activations = ActivationFunctions()
        self.differentials = DifferentialFunctions()
        self.initializer = Initializer()
        self.arithmetic = ArithmeticOperations()

        # Activation function names
        self.activation_functions = {
            'sigmoid': self.activations.sigmoid,
            'tanh': np.tanh,
            'relu': self.activations.relu,
            'linear': lambda x: x,  # Identity function
            'softmax': self.activations.softmax
        }

        # Derivative function names
        self.derivative_functions = {
            'sigmoid': self.differentials.sigmoid_derivative,
            'tanh': self.differentials.tanh_derivative,
            'relu': self.differentials.relu_derivative,
            'linear': self.differentials.identity_derivative,
            'softmax': lambda x: 1  # Placeholder, softmax typically used in output layer
        }

    def forward_propagation(self, weights, biases, input_data):
        activations = [input_data]
        pre_activations = []

        for i in range(1, self.layers):
            bias = biases[i]
            a = bias + np.dot(weights[i], activations[i - 1])
            pre_activations.append(a)

            activation_func = self.activation_functions.get(self.activation.lower(), lambda x: x) # Use .lower() for case-insensitivity
            h = activation_func(a)

            activations.append(h)

        # Output layer (Softmax)
        bias = biases[self.layers]
        a = bias + np.dot(weights[self.layers], activations[self.layers - 1])
        pre_activations.append(a)

        y_hat = self.activations.softmax(a)
        activations.append(y_hat)

        return pre_activations, activations

    def backward_propagation(self, pre_activations, activations, weights, biases, y):
        delA = [None] * (self.layers + 1)
        delW = [None] * (self.layers + 1)
        delb = [None] * (self.layers + 1)
        delh = [None] * (self.layers + 1)

        if self.config['loss'] == 'cross_entropy':
            delA[self.layers] = -(y - activations[self.layers])
        else:
            delA[self.layers] = -(y - activations[self.layers]) * (activations[self.layers]) * (1 - activations[self.layers])

        for k in range(self.layers, 0, -1):
            delW[k] = np.matmul(delA[k], activations[k - 1].T)
            delb[k] = np.sum(delA[k], axis=1, keepdims=True)
            delh[k - 1] = weights[k].T @ delA[k]

            if k > 1:
                derivative_func = self.derivative_functions.get(self.activation.lower(), lambda x: np.ones_like(x))  # Default to identity
                delA[k - 1] = np.multiply(delh[k - 1], derivative_func(pre_activations[k - 2])) # k-2 to access pre_activation one level before

        return delW, delb

    def calculate_validation_loss(self, weights, biases):
        total_loss = 0.0
        correct_predictions = 0

        for j in range(0, len(x_val) // self.batch_size):
            h0 = x_val[j * self.batch_size:(j + 1) * self.batch_size].T
            A, H = self.forward_propagation(weights, biases, h0)
            y = y_val[j * self.batch_size:(j + 1) * self.batch_size].T  # Transpose y to match H
            yp = H[self.layers]

            for itr in range(self.batch_size):
                if self.config['loss'] == 'cross_entropy':
                    total_loss -= np.sum(y[:, itr] * np.log(yp[:, itr] + 1e-20))
                else:
                    total_loss += np.sum((y[:, itr] - yp[:, itr]) ** 2)

                if np.argmax(y[:, itr]) == np.argmax(yp[:, itr]):
                    correct_predictions += 1

        return total_loss / len(x_val), correct_predictions * 100 / len(x_val)

    def stochastic_gradient_descent(self):
        epochs = self.config['epochs']
        weights, biases = self.initializer.initialize_weights_biases(self.layers - 1, self.neurons_per_layer, self.initialization)

        training_loss = []
        validation_loss = []
        training_accuracy = []
        validation_accuracy = []

        for i in range(epochs):
            total_loss = 0.0
            correct_predictions = 0

            for j in range(0, len(x_train) // self.batch_size):
                h0 = x_train[j * self.batch_size:(j + 1) * self.batch_size].T
                A, H = self.forward_propagation(weights, biases, h0)
                y = y_train[j * self.batch_size:(j + 1) * self.batch_size].T
                yp = H[self.layers]

                for itr in range(self.batch_size):
                    if self.config['loss'] == 'cross_entropy':
                        total_loss -= np.sum(y[:, itr] * np.log(yp[:, itr] + 1e-20))
                    else:
                        total_loss += np.sum((y[:, itr] - yp[:, itr]) ** 2)

                    if np.argmax(y[:, itr]) == np.argmax(yp[:, itr]):
                        correct_predictions += 1

                delW, delb = self.backward_propagation(A, H, weights, biases, y)

                weights = self.arithmetic.subtract(weights, delW, self.learning_rate)
                biases = self.arithmetic.subtract(biases, delb, self.learning_rate)

            training_loss.append(total_loss / len(x_train))
            training_accuracy.append(correct_predictions * 100 / len(x_train))
            val_loss, val_acc = self.calculate_validation_loss(weights, biases)
            validation_loss.append(val_loss)
            validation_accuracy.append(val_acc)

            wandb.log({
                "training_loss": training_loss[-1],
                "validation_loss": validation_loss[-1],
                "training_accuracy": training_accuracy[-1],
                "validation_accuracy": validation_accuracy[-1],
                'epoch': i + 1
            })

            print('******************************************************************************************')
            print('epoch : ', i + 1)
            print('Training accuracy :', training_accuracy[-1], 'Training Loss :', training_loss[-1])
            print('Validation accuracy :', validation_accuracy[-1], 'Validation Loss :', validation_loss[-1])
            print('******************************************************************************************')

        return weights, biases

    def Momentum_Gradient_descent(self):
        epochs = self.config['epochs']
        I = Initializer()

        # Initialize weights and biases based on user preference
        weights, biases = self.initializer.initialize_weights_biases(self.layers - 1, self.neurons_per_layer, self.initialization)

        # Initialize previous updates to zero
        prev_uW = [np.zeros_like(w) for w in weights]
        prev_ub = [np.zeros_like(b) for b in biases]

        beta = self.config['beta']

        training_loss = []
        validation_loss = []
        training_accuracy = []
        validation_accuracy = []

        PMA = ArithmeticOperations()

        for epoch in range(epochs):
            total_loss = 0.0
            correct_predictions = 0

            for j in range(0, len(x_train) // self.batch_size):
                h0 = x_train[j * self.batch_size:(j + 1) * self.batch_size].T
                A, H = self.forward_propagation(weights, biases, h0)
                y = y_train[j * self.batch_size:(j + 1) * self.batch_size].T
                yp = H[self.layers]

                for itr in range(self.batch_size):
                    if self.config['loss'] == 'cross_entropy':
                        total_loss -= np.sum(y[:, itr] * np.log(yp[:, itr] + 1e-20))
                    else:
                        total_loss += np.sum((y[:, itr] - yp[:, itr]) ** 2)

                    if np.argmax(y[:, itr]) == np.argmax(yp[:, itr]):
                        correct_predictions += 1

                delW, delb = self.backward_propagation(A, H, weights, biases, y)

                # Apply momentum update
                for k in range(1, self.layers + 1):  # Iterate through layers
                    prev_uW[k] = beta * prev_uW[k] + delW[k]
                    prev_ub[k] = beta * prev_ub[k] + delb[k]

                # Update weights and biases
                weights = PMA.subtract(weights, prev_uW, self.learning_rate)
                biases = PMA.subtract(biases, prev_ub, self.learning_rate)

            training_loss.append(total_loss / len(x_train))
            training_accuracy.append(correct_predictions * 100 / len(x_train))
            val_loss, val_acc = self.calculate_validation_loss(weights, biases)
            validation_loss.append(val_loss)
            validation_accuracy.append(val_acc)

            wandb.log({
                "training_loss": training_loss[-1],
                "validation_loss": validation_loss[-1],
                "training_accuracy": training_accuracy[-1],
                "validation_accuracy": validation_accuracy[-1],
                'epoch': epoch + 1
            })

            print(f"Epoch {epoch + 1}: Training Acc = {training_accuracy[-1]:.2f}%, Training Loss = {training_loss[-1]:.4f}")
            print(f"Validation Acc = {validation_accuracy[-1]:.2f}%, Validation Loss = {validation_loss[-1]:.4f}")
            print("=" * 100)

        return weights, biases

    def NAG_descent(self):
        epochs = self.config['epochs']
        I = Initializer()

        # Initialize weights and biases
        weights, biases = self.initializer.initialize_weights_biases(self.layers - 1, self.neurons_per_layer, self.initialization)

        # Initialize velocity vectors to zero
        vW = [np.zeros_like(w) for w in weights]
        vb = [np.zeros_like(b) for b in biases]

        beta = self.config['beta']

        training_loss = []
        validation_loss = []
        training_accuracy = []
        validation_accuracy = []

        ASA = ArithmeticOperations()

        for epoch in range(epochs):
            total_loss = 0.0
            correct_predictions = 0

            for j in range(0, len(x_train) // self.batch_size):
                h0 = x_train[j * self.batch_size:(j + 1) * self.batch_size].T

                # Lookahead weights and biases
                weights_lookahead = ASA.add(weights, [beta * vw for vw in vW])
                biases_lookahead = ASA.add(biases, [beta * vb for vb in vb])

                # Forward propagation with lookahead parameters
                A, H = self.forward_propagation(weights_lookahead, biases_lookahead, h0)

                y = y_train[j * self.batch_size:(j + 1) * self.batch_size].T
                yp = H[self.layers]

                for itr in range(self.batch_size):
                    if self.config['loss'] == 'cross_entropy':
                        total_loss -= np.sum(y[:, itr] * np.log(yp[:, itr] + 1e-20))
                    else:
                        total_loss += np.sum((y[:, itr] - yp[:, itr]) ** 2)

                    if np.argmax(y[:, itr]) == np.argmax(yp[:, itr]):
                        correct_predictions += 1

                # Backward propagation with lookahead parameters
                delW, delb = self.backward_propagation(A, H, weights_lookahead, biases_lookahead, y)

                # Update velocities
                for k in range(1, self.layers + 1):
                    vW[k] = beta * vW[k] + self.learning_rate * delW[k]
                    vb[k] = beta * vb[k] + self.learning_rate * delb[k]

                # Update weights and biases
                weights = ASA.subtract(weights, vW, 1)
                biases = ASA.subtract(biases, vb, 1)

            training_loss.append(total_loss / len(x_train))
            training_accuracy.append(correct_predictions * 100 / len(x_train))
            val_loss, val_acc = self.calculate_validation_loss(weights, biases)
            validation_loss.append(val_loss)
            validation_accuracy.append(val_acc)

            wandb.log({
                "training_loss": training_loss[-1],
                "validation_loss": validation_loss[-1],
                "training_accuracy": training_accuracy[-1],
                "validation_accuracy": validation_accuracy[-1],
                'epoch': epoch + 1
            })

            print(f"Epoch {epoch + 1}: Training Acc = {training_accuracy[-1]:.2f}%, Training Loss = {training_loss[-1]:.4f}")
            print(f"Validation Acc = {validation_accuracy[-1]:.2f}%, Validation Loss = {validation_loss[-1]:.4f}")
            print("=" * 100)

        return weights, biases

    def RMSprop(self):
        epochs = self.config['epochs']
        I = Initializer()

        # Initialize weights and biases
        weights, biases = self.initializer.initialize_weights_biases(self.layers - 1, self.neurons_per_layer, self.initialization)

        # Initialize squared gradient accumulation
        L = [np.zeros_like(w) for w in weights]

        eps = self.config['eps']
        gamma = self.config['gamma']

        training_loss = []
        validation_loss = []
        training_accuracy = []
        validation_accuracy = []

        RMS = ArithmeticOperations()

        for epoch in range(epochs):
            total_loss = 0.0
            correct_predictions = 0

            for j in range(0, len(x_train) // self.batch_size):
                h0 = x_train[j * self.batch_size:(j + 1) * self.batch_size].T
                A, H = self.forward_propagation(weights, biases, h0)

                y = y_train[j * self.batch_size:(j + 1) * self.batch_size].T
                yp = H[self.layers]

                for itr in range(self.batch_size):
                    if self.config['loss'] == 'cross_entropy':
                        total_loss -= np.sum(y[:, itr] * np.log(yp[:, itr] + 1e-20))
                    else:
                        total_loss += np.sum((y[:, itr] - yp[:, itr]) ** 2)

                    if np.argmax(y[:, itr]) == np.argmax(yp[:, itr]):
                        correct_predictions += 1

                delW, delb = self.backward_propagation(A, H, weights, biases, y)

                # Accumulate squared gradients
                for k in range(1, self.layers + 1):
                    L[k] = gamma * L[k] + (1 - gamma) * (delW[k] ** 2)

                # Update weights and biases using RMSprop
                weights = RMS.rmsprop_subtract(weights, delW, L, eps, self.learning_rate)
                biases = RMS.rmsprop_subtract(biases, delb, L, eps, self.learning_rate)

            training_loss.append(total_loss / len(x_train))
            training_accuracy.append(correct_predictions * 100 / len(x_train))
            val_loss, val_acc = self.calculate_validation_loss(weights, biases)
            validation_loss.append(val_loss)
            validation_accuracy.append(val_acc)

            wandb.log({
                "training_loss": training_loss[-1],
                "validation_loss": validation_loss[-1],
                "training_accuracy": training_accuracy[-1],
                "validation_accuracy": validation_accuracy[-1],
                'epoch': epoch + 1
            })

            print(f"Epoch {epoch + 1}: Training Acc = {training_accuracy[-1]:.2f}%, Training Loss = {training_loss[-1]:.4f}")
            print(f"Validation Acc = {validation_accuracy[-1]:.2f}%, Validation Loss = {validation_loss[-1]:.4f}")
            print("=" * 100)

        return weights, biases

    def Adam(self):
        epochs = self.config['epochs']
        I = Initializer()

        # Initialize weights and biases
        weights, biases = self.initializer.initialize_weights_biases(self.layers - 1, self.neurons_per_layer, self.initialization)

        # Initialize first and second moment estimates
        mV = [np.zeros_like(w) for w in weights]
        vV = [np.zeros_like(w) for w in weights]

        beta1 = self.config['beta1']
        beta2 = self.config['beta2']
        eps = self.config['eps']

        training_loss = []
        validation_loss = []
        training_accuracy = []
        validation_accuracy = []

        AdamOp = ArithmeticOperations()

        for epoch in range(epochs):
            # Initialize bias-corrected first and second moment estimates
            mV_hat = [np.zeros_like(w) for w in weights]
            vV_hat = [np.zeros_like(w) for w in weights]

            total_loss = 0.0
            correct_predictions = 0

            for j in range(0, len(x_train) // self.batch_size):
                h0 = x_train[j * self.batch_size:(j + 1) * self.batch_size].T
                A, H = self.forward_propagation(weights, biases, h0)

                y = y_train[j * self.batch_size:(j + 1) * self.batch_size].T
                yp = H[self.layers]

                for itr in range(self.batch_size):
                    if self.config['loss'] == 'cross_entropy':
                        total_loss -= np.sum(y[:, itr] * np.log(yp[:, itr] + 1e-20))
                    else:
                        total_loss += np.sum((y[:, itr] - yp[:, itr]) ** 2)

                    if np.argmax(y[:, itr]) == np.argmax(yp[:, itr]):
                        correct_predictions += 1

                delW, delb = self.backward_propagation(A, H, weights, biases, y)

                # Update biased first and second moment estimates
                for k in range(1, self.layers + 1):
                    mV[k] = beta1 * mV[k] + (1 - beta1) * delW[k]
                    vV[k] = beta2 * vV[k] + (1 - beta2) * (delW[k] ** 2)

                    # Compute bias-corrected estimates
                    mV_hat[k] = mV[k] / (1 - beta1 ** (epoch + 1))
                    vV_hat[k] = vV[k] / (1 - beta2 ** (epoch + 1))

                # Update weights and biases using Adam
                weights = AdamOp.adam_subtract(weights, mV_hat, vV_hat, eps, self.learning_rate)
                biases = AdamOp.adam_subtract(biases, mV_hat, vV_hat, eps, self.learning_rate)

            training_loss.append(total_loss / len(x_train))
            training_accuracy.append(correct_predictions * 100 / len(x_train))
            val_loss, val_acc = self.calculate_validation_loss(weights, biases)
            validation_loss.append(val_loss)
            validation_accuracy.append(val_acc)

            wandb.log({
                "training_loss": training_loss[-1],
                "validation_loss": validation_loss[-1],
                "training_accuracy": training_accuracy[-1],
                "validation_accuracy": validation_accuracy[-1],
                'epoch': epoch + 1
            })

            print(f"Epoch {epoch + 1}: Training Acc = {training_accuracy[-1]:.2f}%, Training Loss = {training_loss[-1]:.4f}")
            print(f"Validation Acc = {validation_accuracy[-1]:.2f}%, Validation Loss = {validation_loss[-1]:.4f}")
            print("=" * 100)

        return weights, biases

    def Nadam(self):
        epochs = self.config['epochs']
        I = Initializer()

        # Initialize weights and biases
        weights, biases = self.initializer.initialize_weights_biases(self.layers - 1, self.neurons_per_layer, self.initialization)

        # Initialize first and second moment estimates
        mV = [np.zeros_like(w) for w in weights]
        vV = [np.zeros_like(w) for w in weights]

        beta1 = self.config['beta1']
        beta2 = self.config['beta2']
        eps = self.config['eps']

        training_loss = []
        validation_loss = []
        training_accuracy = []
        validation_accuracy = []

        NadamOp = ArithmeticOperations()

        for epoch in range(epochs):
            # Initialize bias-corrected first and second moment estimates
            mV_hat = [np.zeros_like(w) for w in weights]
            vV_hat = [np.zeros_like(w) for w in weights]

            total_loss = 0.0
            correct_predictions = 0

            for j in range(0, len(x_train) // self.batch_size):
                h0 = x_train[j * self.batch_size:(j + 1) * self.batch_size].T
                A, H = self.forward_propagation(weights, biases, h0)

                y = y_train[j * self.batch_size:(j + 1) * self.batch_size].T
                yp = H[self.layers]

                for itr in range(self.batch_size):
                    if self.config['loss'] == 'cross_entropy':
                        total_loss -= np.sum(y[:, itr] * np.log(yp[:, itr] + 1e-20))
                    else:
                        total_loss += np.sum((y[:, itr] - yp[:, itr]) ** 2)

                    if np.argmax(y[:, itr]) == np.argmax(yp[:, itr]):
                        correct_predictions += 1

                delW, delb = self.backward_propagation(A, H, weights, biases, y)

                # Update biased first and second moment estimates
                for k in range(1, self.layers + 1):
                    mV[k] = beta1 * mV[k] + (1 - beta1) * delW[k]
                    vV[k] = beta2 * vV[k] + (1 - beta2) * (delW[k] ** 2)

                    # Compute bias-corrected estimates
                    mV_hat[k] = mV[k] / (1 - beta1 ** (epoch + 1))
                    vV_hat[k] = vV[k] / (1 - beta2 ** (epoch + 1))

                # Nadam update (incorporate momentum directly into the gradient)
                m_scheduled = beta1 * mV_hat[k] + ((1 - beta1) / (1 - beta1**(epoch + 1))) * delW[k]
                weights = NadamOp.adam_subtract(weights, m_scheduled, vV_hat, eps, self.learning_rate) #Using adam_subtract as it has the core logic
                biases = NadamOp.adam_subtract(biases, m_scheduled, vV_hat, eps, self.learning_rate)

            training_loss.append(total_loss / len(x_train))
            training_accuracy.append(correct_predictions * 100 / len(x_train))
            val_loss, val_acc = self.calculate_validation_loss(weights, biases)
            validation_loss.append(val_loss)
            validation_accuracy.append(val_acc)

            wandb.log({
                "training_loss": training_loss[-1],
                "validation_loss": validation_loss[-1],
                "training_accuracy": training_accuracy[-1],
                "validation_accuracy": validation_accuracy[-1],
                'epoch': epoch + 1
            })

            print(f"Epoch {epoch + 1}: Training Acc = {training_accuracy[-1]:.2f}%, Training Loss = {training_loss[-1]:.4f}")
            print(f"Validation Acc = {validation_accuracy[-1]:.2f}%, Validation Loss = {validation_loss[-1]:.4f}")
            print("=" * 100)

        return weights, biases
    def run_models(self):
        run_name = "op_{}_ep_{}_lay_{}_npl_{}_eta_{}_bs_{}_ini_{}_reg_{}_loss_{}_activ_{}".format(
            self.config['optimizer'], self.config['epochs'], self.config['layers'], self.config['neurons_per_layer'],
            self.config['learning_rate'], self.config['batch_size'], self.config['Initialization'],
            self.config['regularization'], self.config['loss'], self.config['activation'])
        wandb.run.name = run_name

        weights, biases = [], []

        if self.config['optimizer'] == 'sgd':
            weights, biases = self.stochastic_gradient_descent()
        # Add other optimizers here...

        return weights, biases

class Prediction:
    def predict_test(self, weights, biases, config):
        predicted_labels = []
        actual_labels = []
        model = NeuralNetwork(784, 10, config)

        for j in range(0, len(x_test) // config['batch_size']):
            h0 = x_test[j * config['batch_size']:(j + 1) * config['batch_size']]
            A, H = model.forward_propagation(weights, biases, h0.T)
            y = y_test[j * config['batch_size']:(j + 1) * config['batch_size']]
            yp = H[config['layers']].T

            for itr in range(config['batch_size']):
                predicted_labels.append(yp[itr])
                actual_labels.append(y[itr])

        predicted_labels = np.array(predicted_labels)
        actual_labels = np.array(actual_labels)

        conf_matrix = confusion_matrix(np.argmax(predicted_labels, axis=1), np.argmax(actual_labels, axis=1))
        acc = np.diag(conf_matrix).sum() / conf_matrix.sum()
        print(conf_matrix)
        print(acc * 100)
        return actual_labels, predicted_labels
